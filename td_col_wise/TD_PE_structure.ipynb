{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Dropout: Column wise target dropout based on the different PE-group sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the PE-wise threshold-based group lasso pruning(Yang, AAAI 2020), they consider the PE-group size as the basic pruning unit then perform the structured pruning to introduce the sparsity into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns;sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report, I'm going to reshpe the 4-D weight tensor into 2-D matrix based on the different PE-group sizes. The PE-size varying from 16 to 2. Follow the Target Dropout paper, we first load the ResNet32 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv_1_3x3.weight, shape: [16, 3, 3, 3]\n",
      "Layer: stage_1.0.conv_a.weight, shape: [16, 16, 3, 3]\n",
      "Layer: stage_1.0.conv_b.weight, shape: [16, 16, 3, 3]\n",
      "Layer: stage_1.1.conv_a.weight, shape: [16, 16, 3, 3]\n",
      "Layer: stage_1.1.conv_b.weight, shape: [16, 16, 3, 3]\n",
      "Layer: stage_1.2.conv_a.weight, shape: [16, 16, 3, 3]\n",
      "Layer: stage_1.2.conv_b.weight, shape: [16, 16, 3, 3]\n",
      "Layer: stage_1.3.conv_a.weight, shape: [16, 16, 3, 3]\n",
      "Layer: stage_1.3.conv_b.weight, shape: [16, 16, 3, 3]\n",
      "Layer: stage_1.4.conv_a.weight, shape: [16, 16, 3, 3]\n",
      "Layer: stage_1.4.conv_b.weight, shape: [16, 16, 3, 3]\n",
      "Layer: stage_2.0.conv_a.weight, shape: [32, 16, 3, 3]\n",
      "Layer: stage_2.0.conv_b.weight, shape: [32, 32, 3, 3]\n",
      "Layer: stage_2.1.conv_a.weight, shape: [32, 32, 3, 3]\n",
      "Layer: stage_2.1.conv_b.weight, shape: [32, 32, 3, 3]\n",
      "Layer: stage_2.2.conv_a.weight, shape: [32, 32, 3, 3]\n",
      "Layer: stage_2.2.conv_b.weight, shape: [32, 32, 3, 3]\n",
      "Layer: stage_2.3.conv_a.weight, shape: [32, 32, 3, 3]\n",
      "Layer: stage_2.3.conv_b.weight, shape: [32, 32, 3, 3]\n",
      "Layer: stage_2.4.conv_a.weight, shape: [32, 32, 3, 3]\n",
      "Layer: stage_2.4.conv_b.weight, shape: [32, 32, 3, 3]\n",
      "Layer: stage_3.0.conv_a.weight, shape: [64, 32, 3, 3]\n",
      "Layer: stage_3.0.conv_b.weight, shape: [64, 64, 3, 3]\n",
      "Layer: stage_3.1.conv_a.weight, shape: [64, 64, 3, 3]\n",
      "Layer: stage_3.1.conv_b.weight, shape: [64, 64, 3, 3]\n",
      "Layer: stage_3.2.conv_a.weight, shape: [64, 64, 3, 3]\n",
      "Layer: stage_3.2.conv_b.weight, shape: [64, 64, 3, 3]\n",
      "Layer: stage_3.3.conv_a.weight, shape: [64, 64, 3, 3]\n",
      "Layer: stage_3.3.conv_b.weight, shape: [64, 64, 3, 3]\n",
      "Layer: stage_3.4.conv_a.weight, shape: [64, 64, 3, 3]\n",
      "Layer: stage_3.4.conv_b.weight, shape: [64, 64, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "check_point = torch.load('./decay0.0002_fp_fflf_resnet32/model_best.pth.tar', map_location='cpu')\n",
    "param = check_point['state_dict']\n",
    "\n",
    "layers = param.items()\n",
    "conv_layers = {}\n",
    "\n",
    "for k,v in layers:\n",
    "    if len(v.size()) == 4:\n",
    "        print(f\"Layer: {k}, shape: {list(v.size())}\")\n",
    "        conv_layers.update({k:v})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the second convolutional layer of the second stage as the example, reshape the 4-D tensor into 2-D matrix based on the different PE-group size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example layer: [32, 32, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "w_l = conv_layers['stage_2.1.conv_a.weight']\n",
    "print(f\"example layer: {list(w_l.size())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweep group size: [2, 4, 8, 16]\n"
     ]
    }
   ],
   "source": [
    "grp_size = [2, 4, 8, 16]\n",
    "print(f\"sweep group size: {grp_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group size=2, shape=[18, 512]\n",
      "group size=4, shape=[36, 256]\n",
      "group size=8, shape=[72, 128]\n",
      "group size=16, shape=[144, 64]\n"
     ]
    }
   ],
   "source": [
    "def reshape_2_2D(input, g):\n",
    "    w_i = input\n",
    "    num_group = w_i.size(0) * w_i.size(1) // g \n",
    "    \n",
    "    reshape_layer = w_i.view(g * w_i.size(2) * w_i.size(3), num_group)  # reshape the weight tensor into 4-D matrix\n",
    "    return reshape_layer\n",
    "\n",
    "for i, g in enumerate(grp_size):\n",
    "    w_i = w_l\n",
    "    \n",
    "    reshape_layer = reshape_2_2D(w_i, g)\n",
    "    print(f\"group size={g}, shape={list(reshape_layer.size())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(input, col_size=4, alpha=0.5, gamma=0.5):\n",
    "    w_i = reshape_2_2D(input, col_size)\n",
    "    print(f\"group size={col_size}, shape={list(w_i.size())}\")\n",
    "    \n",
    "    grp_values = w_i.norm(p=2, dim=0)\n",
    "    print(f'grp values size={grp_values.size()}')\n",
    "    \n",
    "    sorted_col, indices = torch.sort(grp_values.contiguous().view(-1), dim=0)\n",
    "    print(sorted_col.size())\n",
    "\n",
    "    th_idx = int(grp_values.numel() * gamma)\n",
    "    threshold = sorted_col[th_idx]\n",
    "    print(f\"threshold L2 norm: {threshold}, idx={th_idx}\")\n",
    "    \n",
    "    mask_small = 1 - grp_values.gt(threshold).float() # mask for blocks candidates for pruning\n",
    "    mask_dropout = torch.rand_like(grp_values).lt(alpha).float()\n",
    "    \n",
    "    mask_keep = 1 - mask_small * mask_dropout\n",
    "    \n",
    "    mask_keep_2d = mask_keep.expand(w_i.size()) \n",
    "    print(mask_keep)\n",
    "    print(mask_keep_2d[:,3])\n",
    "    \n",
    "    mask_keep_original = mask_keep_2d.resize_as_(input)\n",
    "    return mask_keep_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group size=4, shape=[36, 256]\n",
      "grp values size=torch.Size([256])\n",
      "torch.Size([256])\n",
      "threshold L2 norm: 0.28617072105407715, idx=128\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.4013e-45, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "mask_keep_original_test = forward(w_l)\n",
    "mask_keep_2d_test = mask_keep_original_test.view(36, 256)\n",
    "print(mask_keep_2d_test[:,1].float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block based target dropout\n",
    "block_size = 8\n",
    "\n",
    "def td_forward(input):\n",
    "        # sort blocks by mean absolute value\n",
    "        block_values = F.avg_pool2d(input.data.abs().permute(2,3,0,1),\n",
    "                        kernel_size=(block_size, block_size),\n",
    "                        stride=(block_size, block_size))\n",
    "        \n",
    "        print(f'block values size: {block_values.size()}')\n",
    "        \n",
    "        sorted_block_values, indices = torch.sort(block_values.contiguous().view(-1))\n",
    "        \n",
    "        thre_index = int(block_values.data.numel() * gamma)\n",
    "        threshold = sorted_block_values[thre_index]\n",
    "        mask_small = 1 - block_values.gt(threshold).float() # mask for blocks candidates for pruning\n",
    "        mask_dropout = torch.rand_like(block_values).lt(alpha).float()\n",
    "        mask_keep = 1.0 - mask_small * mask_dropout\n",
    "        \n",
    "        print(f'mask_keep size = {mask_keep.size()}')\n",
    "        \n",
    "        mask_keep_original = F.interpolate(mask_keep, \n",
    "                            scale_factor=(block_size, block_size)).permute(2,3,0,1)\n",
    "        print(f'mask keep original size = {mask_keep_original.size()}')\n",
    "        return mask_keep_original\n",
    "\n",
    "block_mask_w = td_forward(w_l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
